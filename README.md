# INTRODUCTION
																			 
Speech is the fast and best normal way of communicating amongst human. This reality motivate many
researchers to consider speech signal as a quick and effective process to interact between computer and human. It
means the computer should have enough knowledge to identify human voice and speech. Although, there is a
significant improvement in speech recognition but still researcher are away from natural interplay between computer
and human, since computer is not capable of understanding human emotional state. The recognition of emotional
speech aims to recognize the emotional condition of individual utterer by applying his/her voice automatically.
Speech emotion recognition is mostly beneficial for applications, which need human-computer interaction such as
speech synthesis, customer service, education, forensics and medical analysis.
Recognizing of emotional conditions in speech signals are so challengeable area for several reason. First issue of
all speech emotional methods is selecting the best features, which is powerful enough to distinguish between
different emotions. The presence of various language, accent, sentences, speaking style, speakers also add another
difficulty because these characteristics directly change most of the extracted features include pitch, energy.
Furthermore, it is possible to have a more than one specific emotion at the same in the same speech signal, each
emotion correlate with a different part of speech signals. Therefore, defines the boundaries between parts of emotion
in very challenging task. The majority of works are concentrated on monolingual emotion recognition, and making a
presumption that there are no cultural diversity between utterers. However, the multi-lingual emotion classification
process have been considered in some research.
